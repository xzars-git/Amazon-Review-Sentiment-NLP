{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Sentiment Analysis\n",
    "\n",
    "This notebook performs sentiment analysis on Amazon review data from Kaggle using various machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_preprocessing import TextPreprocessor, load_data, create_sentiment_labels\n",
    "from model import SentimentModel\n",
    "from visualization import plot_sentiment_distribution, plot_rating_distribution, plot_word_cloud\n",
    "\n",
    "# Set style for plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your data file\n",
    "data_path = '../data/train.csv'\n",
    "\n",
    "# Load the data\n",
    "df = load_data(data_path)\n",
    "if df is None:\n",
    "    print(\"Failed to load data. Please check the data path.\")\n",
    "else:\n",
    "    print(f\"Data loaded successfully! Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment labels\n",
    "df_labeled = create_sentiment_labels(df, 'Score', 'Text')\n",
    "print(f\"Dataset shape after creating sentiment labels: {df_labeled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "preprocessor = TextPreprocessor()\n",
    "df_processed = preprocessor.preprocess_dataframe(df_labeled, 'Text')\n",
    "print(f\"Dataset shape after preprocessing: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of processed data\n",
    "df_processed[['Text', 'Text_processed', 'sentiment_binary']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment distribution\n",
    "sentiment_fig = plot_sentiment_distribution(df_processed, 'sentiment_binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rating distribution\n",
    "rating_fig = plot_rating_distribution(df_labeled, 'Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud for positive reviews\n",
    "positive_text = ' '.join(df_processed[df_processed['sentiment_binary'] == 1]['Text_processed'])\n",
    "positive_wordcloud = plot_word_cloud(positive_text, \"Word Cloud of Positive Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud for negative reviews\n",
    "negative_text = ' '.join(df_processed[df_processed['sentiment_binary'] == 0]['Text_processed'])\n",
    "negative_wordcloud = plot_word_cloud(negative_text, \"Word Cloud of Negative Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Logistic Regression model\n",
    "lr_model = SentimentModel(model_type='logistic_regression')\n",
    "X_train, X_test, y_train, y_test = lr_model.prepare_data(\n",
    "    df_processed, 'Text_processed', 'sentiment_binary'\n",
    ")\n",
    "lr_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression model\n",
    "lr_results = lr_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Naive Bayes model\n",
    "nb_model = SentimentModel(model_type='naive_bayes')\n",
    "nb_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes model\n",
    "nb_results = nb_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train SVM model\n",
    "svm_model = SentimentModel(model_type='svm')\n",
    "svm_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SVM model\n",
    "svm_results = svm_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Random Forest model\n",
    "rf_model = SentimentModel(model_type='random_forest')\n",
    "rf_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest model\n",
    "rf_results = rf_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model accuracies\n",
    "model_names = ['Logistic Regression', 'Naive Bayes', 'SVM', 'Random Forest']\n",
    "accuracies = [lr_results['accuracy'], nb_results['accuracy'], \n",
    "              svm_results['accuracy'], rf_results['accuracy']]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "\n",
    "# Sort by accuracy\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Accuracy', y='Model', data=comparison_df)\n",
    "plt.title('Model Accuracy Comparison', fontsize=16)\n",
    "plt.xlabel('Accuracy', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for i, acc in enumerate(comparison_df['Accuracy']):\n",
    "    plt.text(acc + 0.01, i, f\"{acc:.4f}\", ha='left', va='center', fontsize=12)\n",
    "\n",
    "plt.xlim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Custom Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model for predictions\n",
    "best_model = lr_model  # Change this to the best performing model\n",
    "\n",
    "# Test with custom reviews\n",
    "test_reviews = [\n",
    "    \"This product is amazing! I love it so much.\",\n",
    "    \"Terrible product. Waste of money. Would not recommend.\",\n",
    "    \"It's okay, not great but not terrible either.\",\n",
    "    \"I would definitely buy this again. Highly recommended!\",\n",
    "    \"Poor quality, broke after just one week of use.\"\n",
    "]\n",
    "\n",
    "# Preprocess the test reviews\n",
    "preprocessed_reviews = [preprocessor.preprocess_text(review) for review in test_reviews]\n",
    "\n",
    "# Make predictions\n",
    "predictions = [best_model.predict(review) for review in preprocessed_reviews]\n",
    "sentiments = ['positive' if pred == 1 else 'negative' for pred in predictions]\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Review': test_reviews,\n",
    "    'Predicted Sentiment': sentiments\n",
    "})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "model_path = '../models/best_sentiment_model.pkl'\n",
    "best_model.save_model(model_path)\n",
    "print(f\"Best model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
