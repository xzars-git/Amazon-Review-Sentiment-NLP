{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Sentiment Analysis
",
    "
",
    "This notebook performs sentiment analysis on Amazon review data from Kaggle using various machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd
",
    "import numpy as np
",
    "import matplotlib.pyplot as plt
",
    "import seaborn as sns
",
    "from sklearn.model_selection import train_test_split
",
    "from sklearn.feature_extraction.text import TfidfVectorizer
",
    "from sklearn.linear_model import LogisticRegression
",
    "from sklearn.naive_bayes import MultinomialNB
",
    "from sklearn.svm import SVC
",
    "from sklearn.ensemble import RandomForestClassifier
",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
",
    "import sys
",
    "sys.path.append('../src')
",
    "from data_preprocessing import TextPreprocessor, load_data, create_sentiment_labels
",
    "from model import SentimentModel
",
    "from visualization import plot_sentiment_distribution, plot_rating_distribution, plot_word_cloud
",
    "
",
    "# Set style for plots
",
    "sns.set(style="whitegrid")
",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your data file
",
    "data_path = '../data/train.csv'
",
    "
",
    "# Load the data
",
    "df = load_data(data_path)
",
    "if df is None:
",
    "    print("Failed to load data. Please check the data path.")
",
    "else:
",
    "    print(f"Data loaded successfully! Shape: {df.shape}")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment labels
",
    "df_labeled = create_sentiment_labels(df, 'Score', 'Text')
",
    "print(f"Dataset shape after creating sentiment labels: {df_labeled.shape}")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text
",
    "preprocessor = TextPreprocessor()
",
    "df_processed = preprocessor.preprocess_dataframe(df_labeled, 'Text')
",
    "print(f"Dataset shape after preprocessing: {df_processed.shape}")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of processed data
",
    "df_processed[['Text', 'Text_processed', 'sentiment_binary']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment distribution
",
    "sentiment_fig = plot_sentiment_distribution(df_processed, 'sentiment_binary')
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rating distribution
",
    "rating_fig = plot_rating_distribution(df_labeled, 'Score')
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud for positive reviews
",
    "positive_text = ' '.join(df_processed[df_processed['sentiment_binary'] == 1]['Text_processed'])
",
    "positive_wordcloud = plot_word_cloud(positive_text, "Word Cloud of Positive Reviews")
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud for negative reviews
",
    "negative_text = ' '.join(df_processed[df_processed['sentiment_binary'] == 0]['Text_processed'])
",
    "negative_wordcloud = plot_word_cloud(negative_text, "Word Cloud of Negative Reviews")
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Logistic Regression model
",
    "lr_model = SentimentModel(model_type='logistic_regression')
",
    "X_train, X_test, y_train, y_test = lr_model.prepare_data(
",
    "    df_processed, 'Text_processed', 'sentiment_binary'
",
    ")
",
    "lr_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression model
",
    "lr_results = lr_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Naive Bayes model
",
    "nb_model = SentimentModel(model_type='naive_bayes')
",
    "nb_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes model
",
    "nb_results = nb_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train SVM model
",
    "svm_model = SentimentModel(model_type='svm')
",
    "svm_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SVM model
",
    "svm_results = svm_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Random Forest model
",
    "rf_model = SentimentModel(model_type='random_forest')
",
    "rf_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest model
",
    "rf_results = rf_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model accuracies
",
    "model_names = ['Logistic Regression', 'Naive Bayes', 'SVM', 'Random Forest']
",
    "accuracies = [lr_results['accuracy'], nb_results['accuracy'], 
",
    "              svm_results['accuracy'], rf_results['accuracy']]
",
    "
",
    "# Create a DataFrame for comparison
",
    "comparison_df = pd.DataFrame({
",
    "    'Model': model_names,
",
    "    'Accuracy': accuracies
",
    "})
",
    "
",
    "# Sort by accuracy
",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)
",
    "
",
    "# Plot model comparison
",
    "plt.figure(figsize=(10, 6))
",
    "sns.barplot(x='Accuracy', y='Model', data=comparison_df)
",
    "plt.title('Model Accuracy Comparison', fontsize=16)
",
    "plt.xlabel('Accuracy', fontsize=12)
",
    "plt.ylabel('Model', fontsize=12)
",
    "
",
    "# Add accuracy labels on bars
",
    "for i, acc in enumerate(comparison_df['Accuracy']):
",
    "    plt.text(acc + 0.01, i, f"{acc:.4f}", ha='left', va='center', fontsize=12)
",
    "
",
    "plt.xlim(0, 1.0)
",
    "plt.tight_layout()
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Custom Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model for predictions
",
    "best_model = lr_model  # Change this to the best performing model
",
    "
",
    "# Test with custom reviews
",
    "test_reviews = [
",
    "    "This product is amazing! I love it so much.",
",
    "    "Terrible product. Waste of money. Would not recommend.",
",
    "    "It's okay, not great but not terrible either.",
",
    "    "I would definitely buy this again. Highly recommended!",
",
    "    "Poor quality, broke after just one week of use."
",
    "]
",
    "
",
    "# Preprocess the test reviews
",
    "preprocessed_reviews = [preprocessor.preprocess_text(review) for review in test_reviews]
",
    "
",
    "# Make predictions
",
    "predictions = [best_model.predict(review) for review in preprocessed_reviews]
",
    "sentiments = ['positive' if pred == 1 else 'negative' for pred in predictions]
",
    "
",
    "# Display results
",
    "results_df = pd.DataFrame({
",
    "    'Review': test_reviews,
",
    "    'Predicted Sentiment': sentiments
",
    "})
",
    "
",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model
",
    "model_path = '../models/best_sentiment_model.pkl'
",
    "best_model.save_model(model_path)
",
    "print(f"Best model saved to {model_path}")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
